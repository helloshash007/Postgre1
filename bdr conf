Here’s a detailed Confluence page draft for your EDB BDR solution. You can copy-paste this directly into your Confluence and format with headings, code blocks, info panels, diagrams, and tables for visual clarity.


---

🔄 EDB BDR (Bi-Directional Replication) Solution Overview

📌 Introduction

This document provides a comprehensive overview of EDB BDR (Bi-Directional Replication) as a high-availability, multi-master replication solution for PostgreSQL. It is designed to support mission-critical workloads with high write/read scalability, low downtime, and active-active database configurations.


---

🧠 What is BDR?

BDR (Bi-Directional Replication) is an advanced logical replication framework provided by EnterpriseDB (EDB). It allows multiple PostgreSQL database nodes to operate in active-active (multi-master) mode where writes can occur on any node, and changes are asynchronously replicated to others.

Key Features:

Multi-master replication

Global DDL replication

Conflict detection and resolution

Support for geo-distributed deployments

Rolling upgrades and failover capabilities



---

🧱 Architecture

Components:

BDR Nodes: Each is a full PostgreSQL instance participating in replication.

Global Sequences: Distributed mechanism for unique value generation across nodes.

Replication Sets: Logical groupings of tables for selective replication.

BDR Worker Processes: Handle decoding, apply, conflict detection, and sync.


Topology Options:

Full Mesh: Every node replicates to all others.

Hierarchical/Hub-Spoke: Centralized replication with leaf nodes.


Architecture Diagram:

┌─────────────┐
       │   Node A    │
       └────▲───▼────┘
            │   │
       ┌────▼───▲────┐
       │   Node B    │
       └────▲───▼────┘
            │   │
       ┌────▼───▲────┐
       │   Node C    │
       └─────────────┘

All nodes can perform writes; changes are replicated via logical decoding asynchronously.


---

✅ Pros and Cons

Pros	Cons

Active-Active replication	Complex setup and management
Supports high availability and zero-downtime upgrades	DDL changes need to be coordinated
Built-in conflict handling	Higher resource requirements
Ideal for distributed environments	Write conflicts may require custom handling
EDB tooling and support	Not suitable for massive uncoordinated writes without planning



---

⚙️ How BDR Handles Huge Data and Heavy Transactions

BDR is designed to scale horizontally. Here’s how it handles large volumes of data and high TPS (Transactions Per Second):

Uses logical replication with decoding slots to extract WAL changes.

Changes are batched and applied asynchronously by dedicated apply workers.

You can enable parallel apply for higher performance.

Use replication sets to isolate high-churn tables.

Avoid unlogged tables or operations bypassing WAL.

Monitor lag and transaction throughput using:

SELECT * FROM bdr.node_status;


Best Practices:

Enable wal_compression for large changes.

Use partitioning for large tables.

Tune max_worker_processes, max_wal_senders, wal_buffers.



---

🧮 Minimum Requirements for Production

These are the recommended minimums for running BDR with enterprise-level workloads:

Workload Size	CPU	RAM	Disk	Notes

Up to 1 TB (moderate TPS)	16 vCPU	64 GB	SSD/NVMe	Optimize WAL
2–4 TB (high TPS)	20–32 vCPU	128–160 GB	High IOPS SSD	Use parallel apply
4 TB+ (batch + OLTP)	32+ vCPU	160 GB+	Dedicated WAL disk	Dedicated network for replication


Additional tuning:

wal_level = logical
max_replication_slots = 20
max_wal_senders = 20
max_worker_processes = 64


---

🔗 Using Logical Replication with External Databases

BDR supports logical replication from its nodes to external PostgreSQL databases. This is helpful if your application needs a separate reporting, audit, or downstream data store.

Use Case:

Application wants to push specific tables to a different PostgreSQL instance that is not part of the BDR cluster.

Steps:

1. On BDR node:



CREATE PUBLICATION app_pub FOR TABLE users, orders;

2. On external target DB:



CREATE SUBSCRIPTION app_sub
  CONNECTION 'host=bdnode1 user=repuser dbname=appdb password=secret'
  PUBLICATION app_pub;

Caution:

Make sure tables on target are read-only.

Avoid cyclic replication or replication back to BDR nodes.



---

🧪 POC Plan: 4 TB Data Replication

Objective:

Evaluate BDR's performance, resilience, and conflict resolution with a 4 TB transactional dataset.

Steps:

1. Provision Nodes:

3 PostgreSQL BDR nodes (20–32 vCPU, 160 GB RAM)

Fast shared or replicated SSD/NVMe storage



2. Install & Configure BDR:

Use bdr_init_copy to set up group

Define replication sets for critical vs bulk tables



3. Load 4 TB Data:

Use pg_restore with parallel jobs

Apply schema and sequence adjustments



4. Test Workload:

Simulate live writes on all nodes

Insert, update, delete operations

High-volume batch jobs + OLTP



5. Failover Test:

Bring down primary node

Promote a secondary

Ensure application failover and minimal downtime



6. Monitor & Validate:

Use EDB tools or Grafana for replication lag

Verify data consistency

Measure throughput and latency



7. Test External Replication:

Setup logical replication to a reporting DB



8. Prepare Report:

Replication lag trends

TPS benchmarks

Failover behavior and DDL sync success





---

✅ Conclusion

EDB BDR offers a mature, scalable, and fault-tolerant PostgreSQL solution for businesses requiring:

Multi-master, high availability

Write scalability

Distributed and hybrid cloud deployments

Smooth upgrade and migration paths


The POC with 4TB of data and real-time workload proves its capability to handle enterprise-grade operations. With proper architecture, it supports not only internal HA but external integrations through logical replication.


---

Would you like this exported as a PDF or converted into Confluence markup (storage format or wiki markup)?

